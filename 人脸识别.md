这次作业，需要使用eigen face和深度学习的办法对人脸进行识别。

## 数据预处理
我们使用的数据集是在自然条件下拍摄的人脸数据，一共有1583人，202792张图片，展示一张效果如下。

![](https://ws2.sinaimg.cn/large/006tNc79ly1fm2epfqpk3j302n04gmwz.jpg)

所以我们需要从该图片截取脸的部分，这时我们可以使用dlib来检测人脸，得到的结果如下。

![](https://ws3.sinaimg.cn/large/006tNc79ly1fm2eqwzj7qj302o02o0si.jpg)

所以我们可以对所有的图片都进行预处理，然后导出保存作为我们的训练集，注意有的图片里面不只一个人，所以我们直接舍弃掉人脸检测出多张脸的图片。

完整代码如下。

```python
import os
import cv2
import dlib

detector = dlib.get_frontal_face_detector()

dir_list = os.listdir('./data/thumbnails_features_deduped_publish/')
for folder in dir_list:
    if not os.path.exists(os.path.join('./data/train/', folder)):
        os.mkdir(os.path.join('./data/train/', folder))
    img_list = os.listdir(os.path.join('./data/thumbnails_features_deduped_publish/', folder))
    for im in img_list:
        if im.split('.')[1] == 'jpg':
            img = cv2.imread(os.path.join('./data/thumbnails_features_deduped_publish/', folder, im))
            dets = detector(img, 1)
            for i, d in enumerate(dets):
                x1 = d.top() if d.top() > 0 else 0
                y1 = d.bottom() if d.bottom() > 0 else 0
                x2 = d.left() if d.left() > 0 else 0
                y2 = d.right() if d.right() > 0 else 0
                face = img[x1:y1, x2:y2, :]
                face = cv2.resize(face, (96, 96)) # resize image to certain size
            if i == 0 and face is not None:
                save_path = os.path.join('./data/train', folder, im)
                cv2.imwrite(save_path, face)
```

## Eigen Face
使用特征脸进行人脸识别是一个非常古老的技术，同时鲁棒性特别差，适用场景也比较小，对数据的量级也有限制，图片不能太大，所以我们选择20个人，每个人选择6张人脸图片，一共是120张图片进行识别

### Algorithm
人脸识别的想法非常简单，我们一步一步给出实现想法。  

1. 将训练集中的每张人脸都展开，然后拼在一起，组成一个大矩阵A。假设每张人脸大小是96 x 96，一共有120张人脸，那么组成的大矩阵A就是120 x 9216。

```python
person_list = os.listdir('./data/train')[0:20]
img = []
label = []
for face in range(len(person_list)):
    img_list = os.listdir(os.path.join('./data/train', person_list[face]))
    for i, im in enumerate(img_list):
        face_img = cv2.imread(os.path.join('./data/train/', person_list[face], im), flags=0)
        face_img = face_img.reshape((-1,)) # 图片展开
        img.append(face_img.astype('float32'))
        label.append(face)
        if i > 4:
            break
img = np.stack(img) # 拼在一起
print(img)
print(img.shape)
```

    [[ 201.   80.  132. ...,   54.   42.   42.]
     [ 216.   81.  127. ...,   54.   53.   53.]
     [ 221.   86.  125. ...,   55.   67.   67.]
     ..., 
     [ 112.  143.  172. ...,   43.   11.   11.]
     [ 112.  140.  177. ...,   42.   18.   18.]
     [ 107.  137.  184. ...,   40.   24.   24.]]

     (9216, 120)

2. 将所有人脸图片对应维度加起来，除以总的图片数，就能够得到一张"平均脸"。

```python
avg_img = np.sum(img, 1) / img.shape[1]
avg_show = avg_img.reshape((96, 96))
plt.imshow(avg_show, 'gray')
```

![](https://ws4.sinaimg.cn/large/006tNc79ly1fm2fiyfohzj3072070mx9.jpg)

3. 将所有的图像都减去平均脸，得到差值图像的矩阵$\Phi$，然后计算协方差矩阵$C = \Phi \Phi^T$，接着对协方差矩阵进行特征值分解，得到最大的前几个特征向量，这些特征向量就是特征脸。

```python
theta = img - avg_img.reshape((-1, 1))
C = np.dot(theta.T, theta) / theta.shape[1]
d, u = np.linalg.eig(C)
v = np.dot(theta, u)

sort_indices = d.argsort()[::-1]
d = d[sort_indices]
v = v[:, sort_indices]

evalues_sum = sum(d)
evalues_count = 0
evalues_energy = 0
for e in d:
    evalues_count += 1
    evalues_energy += e / evalues_sum
    if evalues_energy >= 0.85:
        break

d = d[:evalues_count]
v = v[:, :evalues_count]

plt.imshow(v[:, 3].astype('uint8').reshape((96, 96)), 'gray')

v = v / np.linalg.norm(v, 2) # 规范化
w = np.dot(v.T, theta)
```

![](https://ws2.sinaimg.cn/large/006tNc79ly1fm2flqahuuj3072070q34.jpg)

这里的特征脸效果并不好，因为人脸是通过自然条件下的人的图片进行截取的，所以人脸位置会出现偏差，最好做一下face align。

4. 将训练集和测试集的图片都投影在特征向量上，然后对于每张测试集，找到训练集中与之最接近的训练集，就认为他们是同一张人脸。

```python
test_img = cv2.imread('./data/train/ashley tisdale/3.jpg', flags=0)
test_img = test_img.reshape((-1,))
test_img = test_img - avg_img
S = np.dot(v.T, test_img.reshape((-1, 1)))
diff = w - S
norms = np.linalg.norm(diff, axis=0)
closest_face_id = np.argmin(norms)
```

     42

## 基于深度学习进行人脸识别
目前主流的做法都是使用深度学习进行人脸识别，使用卷积神经网络对人脸图片进行特征提取，然后对提取出来的特征进行距离的比较，一般常用的距离可以是欧几里得距离或者cosine距离。

### Algorithm
下面我们简要描述一下基于卷积神经网络的算法，这里我们使用PyTorch框架来实现整个网络。

网络是基于inception net搭建，参考网上比较成熟的网络，在最后一层进行softmax分类作为loss进行反向传播更新参数，同时全连接层之前的层作为特征提取层，这里我们一共是1580个人得分类，特征是128维。

```python
class netOpenFace(nn.Module):
    def __init__(self, n_class, useCuda=True, gpuDevice=0):
        super(netOpenFace, self).__init__()

        self.gpuDevice = gpuDevice

        self.layer1 = Conv2d(3, 64, (7,7), (2,2), (3,3))
        self.layer2 = BatchNorm(64)
        self.layer3 = nn.ReLU()
        self.layer4 = nn.MaxPool2d((3,3), stride=(2,2), padding=(1,1))
        self.layer5 = CrossMapLRN(5, 0.0001, 0.75, gpuDevice=gpuDevice)
        self.layer6 = Conv2d(64, 64, (1,1), (1,1), (0,0))
        self.layer7 = BatchNorm(64)
        self.layer8 = nn.ReLU()
        self.layer9 = Conv2d(64, 192, (3,3), (1,1), (1,1))
        self.layer10 = BatchNorm(192)
        self.layer11 = nn.ReLU()
        self.layer12 = CrossMapLRN(5, 0.0001, 0.75, gpuDevice=gpuDevice)
        self.layer13 = nn.MaxPool2d((3,3), stride=(2,2), padding=(1,1))
        self.layer14 = Inception(192, (3,5), (1,1), (128,32), (96,16,32,64), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)
        self.layer15 = Inception(256, (3,5), (1,1), (128,64), (96,32,64,64), nn.LPPool2d(2, (3,3), stride=(3,3)), True)
        self.layer16 = Inception(320, (3,5), (2,2), (256,64), (128,32,None,None), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)
        self.layer17 = Inception(640, (3,5), (1,1), (192,64), (96,32,128,256), nn.LPPool2d(2, (3,3), stride=(3,3)), True)
        self.layer18 = Inception(640, (3,5), (2,2), (256,128), (160,64,None,None), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)
        self.layer19 = Inception(1024, (3,), (1,), (384,), (96,96,256), nn.LPPool2d(2, (3,3), stride=(3,3)), True)
        self.layer21 = Inception(736, (3,), (1,), (384,), (96,96,256), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)
        self.layer22 = nn.AvgPool2d((3,3), stride=(1,1), padding=(0,0))
        self.layer25 = Linear(736, 128)
        self.classifier = Linear(736, n_class)

        #
        self.resize1 = nn.UpsamplingNearest2d(scale_factor=3)
        self.resize2 = nn.AvgPool2d(4)

        #
        # self.eval()

        if useCuda:
            self.cuda(gpuDevice)


    def forward(self, input):
        x = input

        if x.data.is_cuda and self.gpuDevice != 0:
            x = x.cuda(self.gpuDevice)

        #
        if x.size()[-1] == 128:
            x = self.resize2(self.resize1(x))

        x = self.layer8(self.layer7(self.layer6(self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x))))))))
        x = self.layer13(self.layer12(self.layer11(self.layer10(self.layer9(x)))))
        x = self.layer14(x)
        x = self.layer15(x)
        x = self.layer16(x)
        x = self.layer17(x)
        x = self.layer18(x)
        x = self.layer19(x)
        x = self.layer21(x)
        x = self.layer22(x)
        x = x.view((-1, 736))
        
        out = self.classifier(x)
        x = self.layer25(x)

        x_norm = torch.sqrt(torch.sum(x**2, 1) + 1e-6)
        x = torch.div(x, x_norm.view(-1, 1).expand_as(x))

        return out, x
```

下面是一部分训练结果

    epoch: 0, loss: 6.180883, acc: 0.034854, time: 308.26
    epoch: 1, loss: 4.770147, acc: 0.134029, time: 303.29
    epoch: 2, loss: 3.698113, acc: 0.274548, time: 301.81
    epoch: 3, loss: 2.836275, acc: 0.417461, time: 302.19
    epoch: 4, loss: 2.201561, acc: 0.067560, time: 302.00
    epoch: 5, loss: 1.739024, acc: 0.126162, time: 301.67
    epoch: 6, loss: 1.390450, acc: 0.194974, time: 301.11
    epoch: 7, loss: 1.122365, acc: 0.248788, time: 301.31
    epoch: 8, loss: 0.904284, acc: 0.291655, time: 301.17
    epoch: 9, loss: 0.741591, acc: 0.323109, time: 301.32
    epoch: 10, loss: 0.613038, acc: 0.348979, time: 301.11
    epoch: 11, loss: 0.507989, acc: 0.370613, time: 301.58
    epoch: 12, loss: 0.432686, acc: 0.386563, time: 301.07
    epoch: 13, loss: 0.377603, acc: 0.398036, time: 301.37
    epoch: 14, loss: 0.327222, acc: 0.410676, time: 301.68
    epoch: 15, loss: 0.292278, acc: 0.418608, time: 301.53
    epoch: 16, loss: 0.269492, acc: 0.424497, time: 301.61
    epoch: 17, loss: 0.250267, acc: 0.429039, time: 301.11

### 测试
训练结束之后，我们可以对任意一张人脸进行检测识别，我们使用lfw的数据集来识别，对于任意一张输入的图片，我们需要提取出其中的面部信息。

```python
def ReadImage(pathname):
    img = cv2.imread(pathname)
    dets = detector(img, 1)
    faces = []
    for i, d in enumerate(dets):
        x1 = d.top() if d.top() > 0 else 0
        y1 = d.bottom() if d.bottom() > 0 else 0
        x2 = d.left() if d.left() > 0 else 0
        y2 = d.right() if d.right() > 0 else 0
        face = img[x1:y1, x2:y2, :]
        face = cv2.resize(face, (96, 96)) # resize image to certain size
        face = face.astype(np.float32)[:, :, ::-1] / 255.
        face = torch.from_numpy(face)
        face = face.permute(2, 0, 1)
        face = face.unsqueeze(0)
        faces.append(face)
    return faces
```

然后我们对于每张面部图片，使用网络进行特征提取，并计算特征之间的cosine距离。

```python
img0 = ReadImage('./data/lfw/Johnny_Unitas/Johnny_Unitas_0001.jpg')
img1 = ReadImage('./data/lfw/Johnny_Unitas/Johnny_Unitas_0002.jpg')
img2 = ReadImage('./data/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg')


net = netOpenFace(1580)
net.load_state_dict(torch.load('./save_net.pth'))
net = net.eval()

x0 = Variable(img0[0], volatile=True).cuda()
x1 = Variable(img1[0], volatile=True).cuda()
x2 = Variable(img2[0], volatile=True).cuda()

out0, f0 = net(x0)
out1, f1 = net(x1)
out2, f2 = net(x2)

torch.dot(f0[0], f1[0]).data[0]
torch.dot(f0[0], f2[0]).data[0]
torch.dot(f1[0], f2[0]).data[0]
```

最后得到的结果如下

     0.6837475299835205
     0.5269231796264648
     0.45731285214424133

可以看到相同的人脸cosine距离更大，说明更相似，而不同的人脸之间cosine距离更小。

### 改进
使用softmax进行分类是一个非常没有效率的做法，因为人脸种类对应于人的种类，这个类别是非常大的，所以使用softmax进行分类并不高效，目前一个更好的loss是triplet loss，每次读入网络的是三张图片，其中两站来自同一个人，另外一张来自于不同的人，triplet loss希望优化的就是网络从图片提取出来的特征之间的距离，这是目前非常流行的做法，因为时间有效，就不再展示。
